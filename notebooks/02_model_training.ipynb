# 02 Model Training

## 1. Import Libraries
```python
import tensorflow as tf
from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, LSTM, add
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pickle, numpy as np, os

with open("../data/Flickr8k_text/tokenizer.pkl", "rb") as f:
    tokenizer = pickle.load(f)

max_length = 37  # Based on preprocessing
vocab_size = len(tokenizer.word_index) + 1

with open("../data/features.pkl", "rb") as f:
    features = pickle.load(f)

with open("../data/Flickr8k_text/captions_mapping.json") as f:
    mapping = json.load(f)

def define_model(vocab_size, max_length):
    inputs1 = Input(shape=(4096,))
    fe1 = Dropout(0.5)(inputs1)
    fe2 = Dense(256, activation='relu')(fe1)

    inputs2 = Input(shape=(max_length,))
    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)
    se2 = Dropout(0.5)(se1)
    se3 = LSTM(256)(se2)

    decoder1 = add([fe2, se3])
    decoder2 = Dense(256, activation='relu')(decoder1)
    outputs = Dense(vocab_size, activation='softmax')(decoder2)

    model = Model(inputs=[inputs1, inputs2], outputs=outputs)
    model.compile(loss='categorical_crossentropy', optimizer='adam')
    return model

model = define_model(vocab_size, max_length)
model.summary()

# Placeholder for generator function and training loop
# Save model
model.save("../models/decoder/lstm_decoder.h5")
